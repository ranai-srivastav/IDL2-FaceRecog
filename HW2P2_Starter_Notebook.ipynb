{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU","colab":{"collapsed_sections":["P5gfdJzx3kG8","EutBOttx3kG8","62j2RAEA3kG8","HavF1IlO3kG9","d5l6iDrZ3kG9","Rsnoo8SU3kG9","da-B39u83kG9"],"gpuType":"T4","provenance":[]},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":115411,"databundleVersionId":13798663,"sourceType":"competition"}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# HW2P2: Image Recognition and Verification","metadata":{"id":"UbXkUQWFLBRF"}},{"cell_type":"markdown","source":"This is the second homework  in 11785: Introduction to Deep Learning. We are trying to tackle the problem of Image Verification. For this, we will need to first train our own CNN model to tackle the problem of classification, consisting of 8631 identities. Using this, we get the face embeddings for different pairs of images and try to identify if the pair of face matches or not.","metadata":{"id":"VMg74_LaLL55"}},{"cell_type":"markdown","source":"# Schedule:\n- Checkpoint Submission (DUE 03 October 2025 @ 11:59PM EST)\n- Kaggle Submission (DUE 10 October 2025 @ 11:59PM EST | Slack Deadline is 17 October 2025 @ 11:59PM EST)\n- Code Submission (DUE 12 October 2025 @ 11:59PM EST OR Day-of Slack submission)\n","metadata":{"id":"7zcx2lHRylKP"}},{"cell_type":"markdown","source":"## Requirement Acknowledgement\nSetting the below flag to True indicates full understanding and acceptance of the following:\n1. Slack days may ONLY be used on P2 FINAL (not checkpoint) submission. I.e. you may use slack days to submit final P2 kaggle scores (such as this one) later on the **SLACK KAGGLE COMPETITION** at the expense of your Slack days.\n2. The final autolab **code submission is due 48 hours after** the conclusion of the Kaggle Deadline (or, the same day as your final kaggle submission).\n3. We will require your kaggle username here, and then we will pull your official PRIVATE kaggle leaderboard score. This submission may result in slight variance in scores/code, but we will check for acceptable discrepancies. Any discrepancies related to modifying the submission code (at the bottom of the notebook) will result in an AIV.\n4. You are NOT allowed to use any code that will pre-load models (such as those from Hugging Face, etc.).\n   You MAY use models described by papers or articles, but you MUST implement them yourself through fundamental PyTorch operations (i.e. Linear, Conv2d, etc.).\n5. You are NOT allowed to use any external data/datasets at ANY point of this assignment.\n6. You may work with teammates to run ablations/experiments, BUT you must submit your OWN code and your OWN results.\n7. Failure to comply with the prior rules will be considered an Academic Integrity Violation (AIV).\n8. Late submissions MUST be submitted through the Slack Kaggle (see writeup for details). Any submissions made to the regular Kaggle after the original deadline will NOT be considered, no matter how many slack days remain for the student.","metadata":{"id":"tcf4nWStyj1g"}},{"cell_type":"code","source":"ACKNOWLEDGED = False #TODO: Only set Acknowledged to True if you have read the above acknowlegements and agree to ALL of them.","metadata":{"id":"zay-09kmxqjQ","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Welcome to the World of Convolutions!\nIn the previous homework, you learned about Multi-Layer Perceptrons (MLPs), the foundation of deep learning. MLPs demonstrated how networks could learn to predict phonemes from speech data, understand patterns, and even approximate complex functions. However, as powerful as MLPs are, they can struggle when dealing with image data. Flattening images into 1D vectors loses spatial information, like how pixels relate to one another. That’s where Convolutional Neural Networks (CNNs) step in, preserving these relationships and taking us one step closer to designing models that \"see\" the world as we do.\n\nIn this homework, you’ll explore CNNs by working on face classification and verification tasks. Imagine training a model to identify a face from a set of known identities (classification) and then using the same model to decide whether two face images belong to the same person (verification).\n\nFor example, given a photo of your friend and a group photo, the classifier would identify your friend’s name, while the verifier would confirm whether two photos depict the same person, even if the person wasn’t part of the training data. CNNs make this possible by extracting hierarchical features from images—from edges and corners in early layers to complex patterns like facial features in deeper layers.\n\nTo succeed, you’ll need to understand the nuances of loss functions. Classification relies on Cross-Entropy Loss, which optimizes the model to predict the correct label. For verification, however, we care about embedding similarity. This is where Triplet Loss or** ArcFace Loss** comes in, pushing embeddings of the same person closer together while separating those of different individuals. For instance, a good verification system would ensure that photos of two siblings (often visually similar) are distinguishable in the embedding space, all while being robust to lighting or pose differences.\n\nThis homework will also emphasize the practical skills needed for real-world deep learning. You’ll preprocess images, apply data augmentation like flipping or cropping to make the model robust to shifts in the images, and experiment with CNN architectures such as ResNet or ConvNeXt. Along the way, you’ll see how designing a model for classification doesn’t automatically make it suitable for verification, teaching you the importance of aligning task objectives with the right loss function. By the end, you’ll have built a robust system that mimics cutting-edge face recognition applications, from unlocking smartphones to verifying IDs. Let’s take this exciting leap together!","metadata":{"id":"WyCRnMfvp83m"}},{"cell_type":"markdown","source":"# **SET-UP**\n\n\nJoin the kaggle competition first (https://www.kaggle.com/t/93e12fee620b40d7a82990b9fe1815c7)","metadata":{"id":"b9EpnOYJ3kG1"}},{"cell_type":"markdown","source":"## **Colab Users**","metadata":{"id":"p5VKNQO43kG2"}},{"cell_type":"markdown","source":"#### Step 1: Environment Setup","metadata":{"id":"pgN5GgiS3kG2"}},{"cell_type":"code","source":"## Config environment\n\nKAGGLE=True\nCOLLAB=False\nPSC=False\nAWS=False\nLOCAL=False\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!nvidia-smi # Run this to see what GPU you have","metadata":{"id":"sQr0ss8w6jVI","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"if COLLAB and \"google.colab\" in sys.modules:\n    from google.colab import drive\n    drive.mount('/content/drive')","metadata":{"id":"PVajPH813kG3","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"if COLLAB and os.path.exists(\"/content\"):\n    %cd /content\n    !pwd","metadata":{"id":"aR_QSfhu3kG4","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install wandb --quiet # Install WandB\n!pip install pytorch_metric_learning --quiet # Install the Pytorch Metric Library\n!pip install torchinfo --quiet # Install torchinfo","metadata":{"id":"0rbYdRn63kG4","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### Step 2: Get Data","metadata":{"id":"E4v1jGLi3kG5"}},{"cell_type":"code","source":"if COLLAB:\n    # TODO: Use the same Kaggle code from HW2P2\n    !pip install --upgrade --force-reinstall --no-deps kaggle==1.7.4.2 --quiet\n    !mkdir /root/.kaggle\n\n    with open(\"/root/.kaggle/kaggle.json\", \"w+\") as f:\n        f.write('{\"username\":\"todo-kaggle username\",\"key\":\"todo-kaggle key\"}')\n        # Put your kaggle username & key here\n\n    !chmod 600 /root/.kaggle/kaggle.json\n    ","metadata":{"id":"jbbKsdqO3kG5","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # Reminder: Make sure you have connected your kaggle API before running this block\n# !mkdir '/content/data'\nif COLLAB:\n    !kaggle competitions download -c 11785-hw-2-p-2-face-verification-fall-2025\n    !unzip -qo '11785-hw-2-p-2-face-verification-fall-2025.zip' -d 'data'","metadata":{"id":"CGBDqD4w3kG5","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## **Kaggle Users**\n\n","metadata":{"id":"E8G-S6Qu3kG6"}},{"cell_type":"markdown","source":"#### Step 1: Environment Setup","metadata":{"id":"Ah--YMLE3kG6"}},{"cell_type":"code","source":"!pwd","metadata":{"id":"wVQgN9Wp3kG6","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"if KAGGLE:\n    !pip install wandb --quiet # Install WandB\n    !pip install pytorch_metric_learning --quiet # Install the Pytorch Metric Library\n    !pip install torchinfo --quiet # Install torchinfo","metadata":{"id":"MFJt80k03kG7","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### Step 2: Get Data\n\nIf you are using Kaggle, follow these steps to add the dataset directly to your notebook:\n1. Join the kaggle competition (https://www.kaggle.com/t/93e12fee620b40d7a82990b9fe1815c7)\n2. Open your **Kaggle Notebook**.  \n3. Navigate to **Notebook → Input**.  \n4. Click **Add Input**.  \n5. Choose \"Competition Datasets\" and \"Your work\", and then you'll be able to see the competiation \"11785-hw-2-p-2-face-verification-fall-2025\"\n6. Click the **➕ (plus sign)** to add the dataset to your notebook.  ","metadata":{"id":"L1yqsDZK3kG7"}},{"cell_type":"markdown","source":"## **PSC Users**","metadata":{"id":"aD8oEOiY3kG8"}},{"cell_type":"markdown","source":"### 1️⃣ **Step 1 Setting Up Your Environment on Bridges2**\n\n❗️⚠️ For this homework, we are **providing shared Datasets and a shared Conda environment** for the entire class.\n\n❗️⚠️ So for PSC users, **do not download the data yourself** and **do not need to manually install the packages**!","metadata":{"id":"g5XKN5jZ3kG8"}},{"cell_type":"markdown","source":"Follow these steps to set up the environment and start a Jupyter notebook on Bridges2:\n\nTo run your notebook more efficiently on PSC, we need to use a **Jupyter Server** hosted on a compute node.\n\nYou can use your prefered way of connecting to the Jupyter Server.\n\n","metadata":{"id":"84eOp-UY3kG8"}},{"cell_type":"markdown","source":"**The recommended way of connecting is:**\n\n#### **Connect in VSCode**\nSSH into Bridges2 and navigate to your **Jet directory** (`Jet/home/<your_username>`). Upload your notebook there, and then connect to the Jupyter Server from that directory.","metadata":{"id":"P5gfdJzx3kG8"}},{"cell_type":"markdown","source":"#### **1. SSH into Bridges2**\n1）Open VS Code and click on the `Extensions` icon in the left sidebar. Make sure the \"**Remote - SSH**\" extension is installed.\n\n2）Open the command palette (**Shift+Command+P** on Mac, **Ctrl+Shift+P** on Windows). A search box will appear at the top center. Choose `\"Remote-SSH: Add New SSH Host\"`, then enter:\n\n```bash\nssh <your_username>@bridges2.psc.edu #change <your_username> to your username\n```\n\nNext, choose `\"/Users/<your_username>/.ssh/config\"` as the config file. A dialog will appear in the bottom right saying \"Host Added\". Click `\"Connect\"`, and then enter your password.\n\n(Note: After adding the host once, you can later use `\"Remote-SSH: Connect to Host\"` and select \"bridges2.psc.edu\" from the list.)\n\n3）Once connected, click `\"Explorer\"` in the left sidebar > \"Open Folder\", and navigate to your home directory under the project grant:\n```bash\n/jet/home/<your_username>  #change <your_username> to your username\n```\n\n4）You can now drag your notebook files directly into the right-hand pane (your remote home directory), or upload them using `scp` into your folder.","metadata":{"id":"EutBOttx3kG8"}},{"cell_type":"markdown","source":"> ❗️⚠️ The following steps should be executed in the **VSCode integrated terminal**.","metadata":{"id":"wzoiexf53kG8"}},{"cell_type":"markdown","source":"#### **2. Navigate to Your Directory**\nMake sure to use this `/jet/home/<your_username>` as your working directory, since all subsequent operations (up to submission) are based on this path.\n```bash\ncd /jet/home/<your_username>  #change <your_username> to your username\n```","metadata":{"id":"62j2RAEA3kG8"}},{"cell_type":"markdown","source":"#### **3. Request a Compute Node**\n```bash\ninteract -p GPU-shared --gres=gpu:v100-32:1 -t 8:00:00 -A cis250019p\n```","metadata":{"id":"HavF1IlO3kG9"}},{"cell_type":"markdown","source":"#### **4. Load the Anaconda Module**\n```bash\nmodule load anaconda3\n```","metadata":{"id":"d5l6iDrZ3kG9"}},{"cell_type":"markdown","source":"#### **5. Activate the provided HW2 Environment**\n```bash\nconda deactivate # First, deactivate any existing Conda environment\nconda activate /ocean/projects/cis250019p/mzhang23/TA/HW2P2/envs/hw2p2_env && export PYTHONNOUSERSITE=1\n```","metadata":{"id":"Rsnoo8SU3kG9"}},{"cell_type":"markdown","source":"#### **6. Start Jupyter Notebook**\nLaunch Jupyter Notebook:\n```bash\njupyter notebook --no-browser --ip=0.0.0.0\n```\n\nGo to **Kernel** → **Select Another Kernel** → **Existing Jupyter Server**\n   Enter the URL of the Jupyter Server:```http://{hostname}:{port}/tree?token={token}```\n   \n   *(Usually, this URL appears in the terminal output after you run `jupyter notebook --no-browser --ip=0.0.0.0`, in a line like:  “Jupyter Server is running at: http://...”)*\n\n   - eg: `http://v011.ib.bridges2.psc.edu:8888/tree?token=e4b302434e68990f28bc2b4ae8d216eb87eecb7090526249`\n\n> **Note**: Replace `{hostname}`, `{port}` and `{token}` with your actual values from the Jupyter output.","metadata":{"id":"da-B39u83kG9"}},{"cell_type":"markdown","source":"#### **7. Navigate to Your Jet Directory**\n\nAfter launching the Jupyter notebook, you can run the cells directly inside the notebook — no need to use the terminal for the remaining steps.\n\nFirst, navigate to your **Jet directory** (`/jet/home/<your_username>`).\n\n️❗️⚠ Please make sure to use your **Jet directory**, not the **Ocean path** — **all HW setup and outputs below are based on this directory**.","metadata":{"id":"M3VWeHyS3kG9"}},{"cell_type":"code","source":"if PSC:\n    #Make sure you are in your directory\n    !pwd #should be /jet/home/<your_username>, if not, uncomment the following line and replace with your actual username:\n    %cd /jet/home/<your_username>  #TODO: replace the \"<your_username>\" to yours","metadata":{"id":"TOnI9xHM3kHC","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 2️⃣ **Step 2: Set up Kaggle API Authentication**","metadata":{"id":"TrdNB7VI3kHD"}},{"cell_type":"code","source":"if PSC:\n    # TODO: Use the same Kaggle code from HW2P2\n    !mkdir /jet/home/<your_username>/.kaggle #TODO: replace the \"<your_username>\" to yours\n\n    with open(\"/jet/home/<your_username>/.kaggle/kaggle.json\", \"w+\") as f: #TODO: replace the \"<your_username>\" to yours\n        f.write('{\"username\":\"<your_username>\",\"key\":\"<your_key>\"}')\n        # TODO: Put your kaggle username & key here\n\n    !chmod 600 /jet/home/<your_username>/.kaggle/kaggle.json #TODO: replace the \"<your_username>\" to yours","metadata":{"id":"iXxci9RA3kHD","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 3️⃣ **Step 3: Get Data**\n\n❗️⚠️ The data used in this assignment is **already stored in a shared, read-only folder, so you do not need to manually download anything**.\n\nInstead, just make sure to replace the dataset path in your notebook code with the correct path from the shared directory.\n\nYou can run the following block to explore the shared directory structure:","metadata":{"id":"yHgQ2mR_3kHD"}},{"cell_type":"code","source":"if PSC:\n    import os\n    data_path = \"/ocean/projects/cis250019p/mzhang23/TA/HW2P2/hw2p2_data/hw2p2_puru_aligned\" #Shared data path, do not need to change the username to yours\n    print(\"Files in shared hw2p2 dataset:\", os.listdir(data_path))","metadata":{"id":"3YNxFJYL3kHD","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"if PSC:\n    !apt-get install tree\n    !tree -L 2 /ocean/projects/cis250019p/mzhang23/TA/HW2P2/hw2p2_data/hw2p2_puru_aligned","metadata":{"id":"sBE_Tqy33kHD","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Imports","metadata":{"id":"Q1XE_p9OsQGp"}},{"cell_type":"code","source":"import torch\nfrom torchsummary import summary\nimport torchvision\nfrom torchvision.utils import make_grid\nfrom torchvision import transforms\nimport torchvision.transforms.v2 as T\nfrom torch.utils.data import DataLoader\nimport torch.nn.functional as F\nimport os\nimport gc\n# from tqdm import tqdm\nfrom tqdm.auto import tqdm\nfrom PIL import Image\nimport numpy as np\nimport pandas as pd\nfrom sklearn.metrics import accuracy_score\nfrom sklearn import metrics as mt\nfrom scipy.optimize import brentq\nfrom scipy.interpolate import interp1d\nimport glob\nimport wandb\nimport matplotlib.pyplot as plt\nfrom pytorch_metric_learning import samplers\nimport csv\n\nDEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\nprint(\"Device: \", DEVICE)\n","metadata":{"id":"GKZw2YsvsPZV","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Config","metadata":{"id":"9OgkfYwP7HVt"}},{"cell_type":"markdown","source":"### Notes:\n\n- You will need to set the root path to your `hw2p2_data` folder in `data: root:`. This will depend on your setup. For eg. if you are following out setup instruction:\n  - `Colab:`: `\"/content/data/hw2p2_puru_aligned\"`\n  - `Kaggle:`: `\"/kaggle/input/11785-hw-2-p-2-face-verification-fall-2025/hw2p2_puru_aligned\"`\n  - `PSC`: `\"/ocean/projects/cis250019p/mzhang23/TA/HW2P2/hw2p2_data/hw2p2_puru_aligned\"`\n\nKindly modify your configurations to suit your ablations and be keen to include your name.","metadata":{"id":"YnmbQ6VntU5D"}},{"cell_type":"code","source":"config = {\n    'batch_size': 1024, # Increase this if your GPU can handle it\n    'lr': 0.1,\n    'epochs': 40, # 20 epochs is recommended ONLY for the early submission - you will have to train for much longer typically.\n    'num_classes': 5000, #Dataset contains 8631 classes for classification, reduce this number if you want to train on a subset, but only for train dataset and not on val dataset\n    'root': \"/kaggle/input/11785-hw-2-p-2-face-verification-fall-2025/hw2p2_puru_aligned\",\n    'cls_data_dir': \"cls_data\", #TODO: Provide path of classification directory\n    'ver_data_dir': \"ver_data\", #TODO: Provide path of verification directory\n    'val_pairs_file': \"val_pairs.txt\", #TODO: Provide path of text file containing val pairs for verification\n    'test_pairs_file': \"test_pairs.txt\", #TODO: Provide path of text file containing test pairs for verification\n    'checkpoint_dir': \"/kaggle/output\", #TODO: Checkpoint directory\n    'augument': False,\n    'h_flip_prob': 0.25,\n    'rand_persp_prob': 0.33\n    # Include other parameters as needed.\n}","metadata":{"id":"CMXkHmFc7G9m","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Dataset","metadata":{"id":"EEAW65sB8Wlp"}},{"cell_type":"code","source":"def create_transforms(image_size: int = 112, augment: bool = True) -> T.Compose:\n    \"\"\"Create transform pipeline for face recognition.\"\"\"\n\n    # Step 1: Basic transformations\n    transform_list = [\n        # Resize the image to the desired size (image_size x image_size)\n        T.Resize((image_size, image_size)),\n\n        # Convert PIL Image to tensor\n        T.ToTensor(),\n\n        # Convert image to float32 and scale the pixel values to [0, 1]\n        T.ToDtype(torch.float32, scale=True),\n    ]\n\n    # Step 2: Data augmentation (optional, based on `augment` argument)\n    if augment:  # This block will be executed if `augment=True`\n        # TODO: Add transformations for data augmentation (e.g., random horizontal flip, rotation, etc.)\n        # HINT: What transforms help faces look more varied?\n        # Think: Does a horizontally flipped face still look like the same person?\n        # What about small rotations or color changes?\n        # Example:\n        transform_list.extend([\n            # T.RandomHorizontalFlip(config[\"h_flip_prob\"]),\n            # T.RandomPerspective(p = config[\"random_persp_prob\"][0], distortion_scale=config[\"random_persp_prob\"][1])\n        ])\n\n    # Step 3: Standard normalization for image recognition tasks\n    # The Normalize transformation requires mean and std values for each channel (R, G, B).\n    # Here, we are normalizing the pixel values to have a mean of 0.5 and std of 0.5 for each channel.\n    transform_list.extend([\n        T.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])  # Standard mean and std for face recognition tasks\n    ])\n\n    # Return the composed transformation pipeline\n    return T.Compose(transform_list)\n","metadata":{"id":"x-jrgnbQyR2s","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Classification Datasets and Dataloaders","metadata":{"id":"ZzxNNzGe8ccB"}},{"cell_type":"code","source":"class ImageDataset(torch.utils.data.Dataset):\n    \"\"\"Custom dataset for loading image-label pairs.\"\"\"\n    def __init__(self, root, transform, num_classes=config['num_classes']):\n        \"\"\"\n        Args:\n            root (str): Path to the directory containing the images folder.\n            transform (callable): Transform to be applied to the images.\n            num_classes (int, optional): Number of classes to keep. If None, keep all classes.\n        \"\"\"\n        self.root = root\n        self.labels_file = os.path.join(self.root, \"labels.txt\")\n        self.transform = transform\n        self.image_paths = []\n        self.labels = []\n        self.classes = set()\n\n        # Read image-label pairs from the file\n        with open(self.labels_file, 'r') as f:\n            lines = f.readlines()\n\n        lines = sorted(lines, key=lambda x: int(x.strip().split(' ')[-1]))\n\n        # Get all unique labels first\n        all_labels = sorted(set(int(line.strip().split(' ')[1]) for line in lines))\n\n         # Select subset of classes if specified\n        if num_classes is not None:\n            selected_classes = set(all_labels[:num_classes])\n        else:\n            selected_classes = set(all_labels)\n\n        # Store image paths and labels with a progress bar\n        for line in tqdm(lines, desc=\"Loading dataset\"):\n            img_path, label = line.strip().split(' ')\n            label = int(label)\n\n            # Only add if label is in selected classes\n            if label in selected_classes:\n                self.image_paths.append(os.path.join(self.root, 'images', img_path))\n                self.labels.append(label)\n                self.classes.add(label)\n\n        assert len(self.image_paths) == len(self.labels), \"Images and labels mismatch!\"\n\n        # Convert classes to a sorted list\n        self.classes = sorted(self.classes)\n\n    def __len__(self):\n        \"\"\"Returns the total number of samples.\"\"\"\n        return len(self.image_paths)\n\n    def __getitem__(self, idx):\n        \"\"\"\n        Args:\n            idx (int): Index of the sample to retrieve.\n\n        Returns:\n            tuple: (transformed image, label)\n        \"\"\"\n        # Load and transform image on-the-fly\n        image = Image.open(self.image_paths[idx]).convert('RGB')\n        image = self.transform(image)\n        label = self.labels[idx]\n        return image, label\n\ngc.collect()","metadata":{"id":"p6Nn57B-7F-i","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# train transforms\ntrain_transforms = create_transforms(augment=config['augument'])\n\n# val transforms\nval_transforms   = create_transforms(augment=False)","metadata":{"id":"f3d1tsYE0pb-","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport sys\n\n\nif not os.path.exists(config['root']):\n    raise ValueError(f\"Path <<{config['root']}>> is not valid\")\n\n# Datasets\ncls_train_dataset = ImageDataset(f\"{config['root']}/{config['cls_data_dir']}/train\", train_transforms)\n# HINT: What dataset class do you use? What folder has your training data?\n# What transforms should training data use?\n\ncls_val_dataset   = ImageDataset(f\"{config['root']}/{config['cls_data_dir']}/dev\", val_transforms)\n# HINT: Same dataset class, but what folder for validation?\n# Should validation use augmentation transforms?\n\ncls_test_dataset  = ImageDataset(f\"{config['root']}/{config['cls_data_dir']}/test\", val_transforms)\n# HINT: What's the pattern here? What folder contains test data?\n\nassert cls_train_dataset.classes == cls_val_dataset.classes == cls_test_dataset.classes, \"Class mismatch!\"\n\n\n# Dataloaders\ncls_train_loader = DataLoader(cls_train_dataset, batch_size=config['batch_size'], shuffle=True,  prefetch_factor=2, num_workers=4, pin_memory=True)\ncls_val_loader   = DataLoader(cls_val_dataset,   batch_size=config['batch_size'], shuffle=False, prefetch_factor=2, num_workers=4, pin_memory=True)\ncls_test_loader  = DataLoader(cls_test_dataset,  batch_size=config['batch_size'], shuffle=False, prefetch_factor=2, num_workers=4, pin_memory=True)","metadata":{"id":"GOJOrQLX02Gh","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Verification Dataset and Datatloaders","metadata":{"id":"mPSk8DyK8htk"}},{"cell_type":"code","source":"class ImagePairDataset(torch.utils.data.Dataset):\n    \"\"\"Custom dataset for loading and transforming image pairs.\"\"\"\n    def __init__(self, root, pairs_file, transform):\n        \"\"\"\n        Args:\n            root (str): Path to the directory containing the images.\n            pairs_file (str): Path to the file containing image pairs and match labels.\n            transform (callable): Transform to be applied to the images.\n        \"\"\"\n        self.root      = root\n        self.transform = transform\n\n        self.matches     = []\n        self.image1_list = []\n        self.image2_list = []\n\n        # Read and load image pairs and match labels\n        with open(pairs_file, 'r') as f:\n            lines = f.readlines()\n\n        for line in tqdm(lines, desc=\"Loading image pairs\"):\n            img_path1, img_path2, match = line.strip().split(' ')\n            img1 = Image.open(os.path.join(self.root, img_path1)).convert('RGB')\n            img2 = Image.open(os.path.join(self.root, img_path2)).convert('RGB')\n\n            self.image1_list.append(img1)\n            self.image2_list.append(img2)\n            self.matches.append(int(match))  # Convert match to integer\n\n        assert len(self.image1_list) == len(self.image2_list) == len(self.matches), \"Image pair mismatch\"\n\n    def __len__(self):\n        \"\"\"Returns the total number of samples.\"\"\"\n        return len(self.image1_list)\n\n    def __getitem__(self, idx):\n        \"\"\"\n        Args:\n            idx (int): Index of the sample to retrieve.\n\n        Returns:\n            tuple: (transformed image1, transformed image2, match label)\n        \"\"\"\n        img1 = self.image1_list[idx]\n        img2 = self.image2_list[idx]\n        match = self.matches[idx]\n        return self.transform(img1), self.transform(img2), match\n","metadata":{"id":"KBleUieO8lwG","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class TestImagePairDataset(torch.utils.data.Dataset):\n    \"\"\"Custom dataset for loading and transforming image pairs.\"\"\"\n    def __init__(self, root, pairs_file, transform):\n        \"\"\"\n        Args:\n            root (str): Path to the directory containing the images.\n            pairs_file (str): Path to the file containing image pairs and match labels.\n            transform (callable): Transform to be applied to the images.\n        \"\"\"\n        self.root      = root\n        self.transform = transform\n\n        self.image1_list = []\n        self.image2_list = []\n\n        # Read and load image pairs and match labels\n        with open(pairs_file, 'r') as f:\n            lines = f.readlines()\n\n        for line in tqdm(lines, desc=\"Loading image pairs\"):\n            img_path1, img_path2 = line.strip().split(' ')\n            img1 = Image.open(os.path.join(self.root, img_path1)).convert('RGB')\n            img2 = Image.open(os.path.join(self.root, img_path2)).convert('RGB')\n\n            self.image1_list.append(img1)\n            self.image2_list.append(img2)\n\n        assert len(self.image1_list) == len(self.image2_list), \"Image pair mismatch\"\n\n    def __len__(self):\n        \"\"\"Returns the total number of samples.\"\"\"\n        return len(self.image1_list)\n\n    def __getitem__(self, idx):\n        \"\"\"\n        Args:\n            idx (int): Index of the sample to retrieve.\n\n        Returns:\n            tuple: (transformed image1, transformed image2, match label)\n        \"\"\"\n        img1 = self.image1_list[idx]\n        img2 = self.image2_list[idx]\n        return self.transform(img1), self.transform(img2)\n","metadata":{"id":"BxXF96_Ys8os","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Datasets\nver_val_dataset  = ImagePairDataset(f'{config[\"root\"]}/ver_data', f'{config[\"root\"]}/val_pairs.txt', val_transforms)\n# HINT: What dataset class handles image pairs? What file lists the validation pairs?\n\nver_test_dataset = TestImagePairDataset(f'{config[\"root\"]}/ver_data', f'{config[\"root\"]}/test_pairs.txt', val_transforms)\n# HINT: Same class, but what file has test pairs? Does it include labels?\n\n# Dataloader\nver_val_loader   = DataLoader(ver_val_dataset,  batch_size=config['batch_size'], shuffle=False, num_workers=8, pin_memory=True)\nver_test_loader  = DataLoader(ver_test_dataset, batch_size=config['batch_size'], shuffle=False, num_workers=8, pin_memory=True)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Create Dataloaders for Image Recognition","metadata":{"id":"2j24TXNo9P97"}},{"cell_type":"markdown","source":"# EDA and Viz","metadata":{"id":"436KzM6u-3A2"}},{"cell_type":"code","source":"# Double-check your dataset/dataloaders work as expected\n\nprint(\"Number of classes    : \", len(cls_train_dataset.classes))\nprint(\"No. of train images  : \", cls_train_dataset.__len__())\nprint(\"Shape of image       : \", cls_train_dataset[0][0].shape)\nprint(\"Batch size           : \", config['batch_size'])\nprint(\"Train batches        : \", cls_train_loader.__len__())\nprint(\"Val batches          : \", cls_val_loader.__len__())\n\n# Feel free to print more things if needed","metadata":{"id":"AhnoHopx-0RB","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"###Classification Dataset Viz","metadata":{"id":"niSJ49lzpHgW"}},{"cell_type":"code","source":"def show_cls_dataset_samples(train_loader, val_loader, test_loader, samples_per_set=8, figsize=(10, 6)):\n    \"\"\"\n    Display samples from train, validation, and test datasets side by side\n\n    Args:\n        train_loader: Training data loader\n        val_loader: Validation data loader\n        test_loader: Test data loader\n        samples_per_set: Number of samples to show from each dataset\n        figsize: Figure size (width, height)\n    \"\"\"\n    def denormalize(x):\n        \"\"\"Denormalize images from [-1, 1] to [0, 1]\"\"\"\n        return x * 0.5 + 0.5\n\n    def get_samples(loader, n):\n        \"\"\"Get n samples from a dataloader\"\"\"\n        batch = next(iter(loader))\n        return batch[0][:n], batch[1][:n]\n\n    # Get samples from each dataset\n    train_imgs, train_labels = get_samples(train_loader, samples_per_set)\n    val_imgs, val_labels = get_samples(val_loader, samples_per_set)\n    test_imgs, test_labels = get_samples(test_loader, samples_per_set)\n\n    # Create figure\n    fig, axes = plt.subplots(3, 1, figsize=figsize)\n\n    # Plot each dataset\n    for idx, (imgs, labels, title) in enumerate([\n        (train_imgs, train_labels, 'Training Samples'),\n        (val_imgs, val_labels, 'Validation Samples'),\n        (test_imgs, test_labels, 'Test Samples')\n    ]):\n\n        # Create grid of images\n        grid = make_grid(denormalize(imgs), nrow=8, padding=2)\n\n        # Display grid\n        axes[idx].imshow(grid.permute(1, 2, 0).cpu())\n        axes[idx].axis('off')\n        axes[idx].set_title(title, fontsize=10)\n\n        # Add class labels below images (with smaller font)\n        grid_width = grid.shape[2]\n        imgs_per_row = min(8, samples_per_set)\n        img_width = grid_width // imgs_per_row\n\n        for i, label in enumerate(labels):\n            col = i % imgs_per_row  # Calculate column position\n            if label<len(train_loader.dataset.classes):\n              class_name = train_loader.dataset.classes[label]\n            else:\n              class_name = f\"Class {label} (Unknown)\"\n            axes[idx].text(col * img_width + img_width/2,\n                         grid.shape[1] + 5,\n                         class_name,\n                         ha='center',\n                         va='top',\n                         fontsize=6,\n                         rotation=45)\n\n    plt.tight_layout()\n    plt.show()\n\nshow_cls_dataset_samples(cls_train_loader, cls_val_loader, cls_test_loader)","metadata":{"id":"fK3t65VU5Qlz","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"###Ver Dataset Viz","metadata":{"id":"KZPVGXOu59Wh"}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport torch\nfrom torchvision.utils import make_grid\n\ndef show_ver_dataset_samples(val_loader, samples_per_set=4, figsize=(12, 8)):\n    \"\"\"\n    Display verification pairs from the validation dataset\n\n    Args:\n        val_loader: Validation data loader\n        samples_per_set: Number of pairs to show from the dataset\n        figsize: Figure size (width, height)\n    \"\"\"\n    def denormalize(x):\n        \"\"\"Denormalize images from [-1, 1] to [0, 1]\"\"\"\n        return x * 0.5 + 0.5\n\n    def get_samples(loader, n):\n        \"\"\"Get n samples from a dataloader\"\"\"\n        batch = next(iter(loader))\n        return batch[0][:n], batch[1][:n], batch[2][:n]\n\n    # Get samples from the validation dataset\n    val_imgs1, val_imgs2, val_labels = get_samples(val_loader, samples_per_set)\n\n    # Create figure and axis\n    fig, ax = plt.subplots(1, 1, figsize=figsize)\n\n    # Create grids for both images in each pair\n    grid1 = make_grid(denormalize(val_imgs1), nrow=samples_per_set, padding=2)\n    grid2 = make_grid(denormalize(val_imgs2), nrow=samples_per_set, padding=2)\n\n    # Combine the grids vertically\n    combined_grid = torch.cat([grid1, grid2], dim=1)\n\n    # Display the combined grid\n    ax.imshow(combined_grid.permute(1, 2, 0).cpu())\n    ax.axis('off')\n    ax.set_title('Validation Pairs', fontsize=10)\n\n    # Determine dimensions for placing the labels\n    grid_width = grid1.shape[2]\n    img_width = grid_width // samples_per_set\n\n    # Add match/non-match labels for each pair\n    for i, label in enumerate(val_labels):\n        match_text = \"✓ Match\" if label == 1 else \"✗ Non-match\"\n        color = 'green' if label == 1 else 'red'\n\n        # Define a background box for the label\n        bbox_props = dict(\n            boxstyle=\"round,pad=0.3\",\n            fc=\"white\",\n            ec=color,\n            alpha=0.8\n        )\n\n        ax.text(i * img_width + img_width / 2,\n                combined_grid.shape[1] + 15,  # Position below the images\n                match_text,\n                ha='center',\n                va='top',\n                fontsize=8,\n                color=color,\n                bbox=bbox_props)\n\n    plt.suptitle(\"Verification Pairs (Top: Image 1, Bottom: Image 2)\", y=1.02)\n    plt.tight_layout()\n    plt.subplots_adjust(bottom=0.05)\n    plt.show()\n\nshow_ver_dataset_samples(ver_val_loader)","metadata":{"id":"6pBN7Z9K5iAM","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Model Architecture","metadata":{"id":"y3TUocDw_JU_"}},{"cell_type":"markdown","source":"FAQ:\n\n**What's a very low early deadline architecture (mandatory early submission)**?\n\n- The very low early deadline architecture is a 5-layer CNN. Keep in mind the parameter limit for this homework is 30M.\n- The first convolutional layer has 64 channels, kernel size 7, and stride 4. The next three have 128, 256, 512 and 1024 channels. Each have kernel size 3 and stride 2. Documentation to make convolutional layers: https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html\n- Think about strided convolutions from the lecture, as convolutions with stride = 1 and downsampling. For strided convolution, what padding do you need for preserving the spatial resolution? (Hint => padding = kernel_size // 2) - Think why?\n- Each convolutional layer is accompanied by a Batchnorm and ReLU layer.\n- Finally, you want to average pool over the spatial dimensions to reduce them to 1 x 1. Use AdaptiveAvgPool2d. Documentation for AdaptiveAvgPool2d: https://pytorch.org/docs/stable/generated/torch.nn.AdaptiveAvgPool2d.html\n- Then, remove (Flatten?) these trivial 1x1 dimensions away.\nLook through https://pytorch.org/docs/stable/nn.html\n\n\n**Why does a very simple network have 4 convolutions**?\n\nInput images are 112x112. Note that each of these convolutions downsample. Downsampling 2x effectively doubles the receptive field, increasing the spatial region each pixel extracts features from. Downsampling 32x is standard for most image models.\n\n**Why does a very simple network have high channel sizes**?\n\nEvery time you downsample 2x, you do 4x less computation (at same channel size). To maintain the same level of computation, you 2x increase # of channels, which increases computation by 4x. So, balances out to same computation. Another intuition is - as you downsample, you lose spatial information. We want to preserve some of it in the channel dimension.\n\n**What is return_feats?**\n\nIt essentially returns the second-to-last-layer features of a given image. It's a \"feature encoding\" of the input image, and you can use it for the verification task. You would use the outputs of the final classification layer for the classification task. You might also find that the classification outputs are sometimes better for verification too - try both.","metadata":{"id":"B4pZEjs2peC4"}},{"cell_type":"code","source":"# TODO: Fill out the model definition below\n\nclass Network(torch.nn.Module):\n\n    def __init__(self, num_classes):\n        super().__init__()\n\n        self.backbone = torch.nn.Sequential( #112x112\n            torch.nn.Conv2d(in_channels=3, out_channels=64, kernel_size=7, stride=4, padding=3, padding_mode=\"zeros\"), #\n            # torch.nn.AdaptiveAvgPool2D((54, 54)),\n            torch.nn.BatchNorm2d(64),\n            torch.nn.ReLU(),\n            torch.nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=2, padding=1, padding_mode=\"zeros\"), #\n            torch.nn.BatchNorm2d(128),\n            torch.nn.ReLU(),\n            # torch.nn.AdaptiveMaxPool2D((25, 25)),\n            torch.nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, stride=2, padding=1, padding_mode=\"zeros\"),\n            torch.nn.BatchNorm2d(256),\n            torch.nn.ReLU(),\n            # torch.nn.AdaptiveMaxPool2D((25, 25)),\n            torch.nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, stride=2, padding=1, padding_mode=\"zeros\"),\n            torch.nn.BatchNorm2d(512),\n            torch.nn.ReLU(),\n            torch.nn.Conv2d(in_channels=512, out_channels=1024, kernel_size=3, stride=2, padding=1, padding_mode=\"zeros\"),\n            torch.nn.BatchNorm2d(1024),\n            torch.nn.ReLU(),\n            torch.nn.AdaptiveAvgPool2d((1,1)),\n            torch.nn.Flatten(),\n            )\n\n        self.cls_layer = torch.nn.Linear(in_features=1024, out_features=num_classes)\n                                            \n\n    def forward(self, x):\n\n        feats = self.backbone(x)\n        out = self.cls_layer(feats)\n\n        return {\"feats\": feats, \"out\": out}\n\n# Initialize your model\nmodel = Network(num_classes=config['num_classes']).to(DEVICE)\nsummary(model, (3, 112, 112))","metadata":{"id":"4LLX2Rki_LzA","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --------------------------------------------------- #\n\n# Defining Loss function\ncriterion = torch.nn.CrossEntropyLoss()\n\n# --------------------------------------------------- #\n\n# Defining Optimizer\noptimizer =  torch.optim.AdamW(model.parameters(), lr=config[\"lr\"])\n\n# --------------------------------------------------- #\n\n# Defining Scheduler\nscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=7, factor=0.1)\n\n# --------------------------------------------------- #\n\n# Initialising mixed-precision training. # Good news. We've already implemented FP16 (Mixed precision training) for you\n# It is useful only in the case of compatible GPUs such as T4/V100\nscaler = torch.cuda.amp.GradScaler()","metadata":{"id":"-d5ZDQfpw7gR","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class AverageMeter:\n    \"\"\"Computes and stores the average and current value\"\"\"\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count","metadata":{"id":"7Ecg0J2sw9jJ","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def accuracy(output, target, topk=(1,)):\n    \"\"\"Computes the accuracy over the k top predictions for the specified values of k\"\"\"\n    maxk = min(max(topk), output.size()[1])\n    batch_size = target.size(0)\n    _, pred = output.topk(maxk, 1, True, True)\n    pred = pred.t()\n    correct = pred.eq(target.reshape(1, -1).expand_as(pred))\n    return [correct[:min(k, maxk)].reshape(-1).float().sum(0) * 100. / batch_size for k in topk]","metadata":{"id":"TqVw0ab0xBKT","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def get_ver_metrics(labels, scores, FPRs):\n    # eer and auc\n    fpr, tpr, _ = mt.roc_curve(labels, scores, pos_label=1)\n    roc_curve = interp1d(fpr, tpr)\n    EER = 100. * brentq(lambda x : 1. - x - roc_curve(x), 0., 1.)\n    AUC = 100. * mt.auc(fpr, tpr)\n\n    # get acc\n    tnr = 1. - fpr\n    pos_num = labels.count(1)\n    neg_num = labels.count(0)\n    ACC = 100. * max(tpr * pos_num + tnr * neg_num) / len(labels)\n\n    # TPR @ FPR\n    if isinstance(FPRs, list):\n        TPRs = [\n            ('TPR@FPR={}'.format(FPR), 100. * roc_curve(float(FPR)))\n            for FPR in FPRs\n        ]\n    else:\n        TPRs = []\n\n    return {\n        'ACC': ACC,\n        'EER': EER,\n        'AUC': AUC,\n        'TPRs': TPRs,\n    }","metadata":{"id":"uNCQjz2RxD5S","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Train and Validation Function","metadata":{"id":"juUbZnP0AEUi"}},{"cell_type":"code","source":"def train_epoch(model, dataloader, optimizer, lr_scheduler, scaler, device, config):\n\n    model.train()\n\n    # metric meters\n    loss_m = AverageMeter()\n    acc_m = AverageMeter()\n\n    # Progress Bar\n    batch_bar = tqdm(total=len(dataloader), dynamic_ncols=True, leave=False, position=0, desc='Train', ncols=5)\n\n    for i, (images, labels) in enumerate(dataloader):\n\n        optimizer.zero_grad() # Zero gradients\n\n        # send to cuda\n        images = images.to(device, non_blocking=True)\n        if isinstance(labels, (tuple, list)):\n            targets1, targets2, lam = labels\n            labels = (targets1.to(device), targets2.to(device), lam)\n        else:\n            labels = labels.to(device, non_blocking=True)\n\n        # forward\n        with torch.cuda.amp.autocast():  # This implements mixed precision. Thats it!\n            outputs = model(images)\n\n            # Use the type of output depending on the loss function you want to use\n            loss = criterion(outputs['out'], labels)\n\n        scaler.scale(loss).backward() # This is a replacement for loss.backward()\n        scaler.step(optimizer) # This is a replacement for optimizer.step()\n        scaler.update()\n        # metrics\n        loss_m.update(loss.item())\n        if 'feats' in outputs:\n            acc = accuracy(outputs['out'], labels)[0].item()\n        else:\n            acc = 0.0\n        acc_m.update(acc)\n\n        # tqdm lets you add some details so you can monitor training as you train.\n        batch_bar.set_postfix(\n            # acc         = \"{:.04f}%\".format(100*accuracy),\n            acc=\"{:.04f}% ({:.04f})\".format(acc, acc_m.avg),\n            loss        = \"{:.04f} ({:.04f})\".format(loss.item(), loss_m.avg),\n            lr          = \"{:.04f}\".format(float(optimizer.param_groups[0]['lr'])))\n\n        batch_bar.update() # Update tqdm bar\n\n    # You may want to call some schedulers inside the train function. What are these?\n    if lr_scheduler is not None:\n        lr_scheduler.step(loss)\n\n    batch_bar.close()\n\n    return acc_m.avg, loss_m.avg","metadata":{"id":"IMnxvQT-AHsu","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"@torch.no_grad()\ndef valid_epoch_cls(model, dataloader, device, config):\n\n    model.eval()\n    batch_bar = tqdm(total=len(dataloader), dynamic_ncols=True, position=0, leave=False, desc='Val Cls.', ncols=5)\n\n    # metric meters\n    loss_m = AverageMeter()\n    acc_m = AverageMeter()\n\n    for i, (images, labels) in enumerate(dataloader):\n\n        # Move images to device\n        images, labels = images.to(device), labels.to(device)\n\n        # Get model outputs\n        with torch.inference_mode():\n            outputs = model(images)\n            loss = criterion(outputs['out'], labels)\n\n        # metrics\n        acc = accuracy(outputs['out'], labels)[0].item()\n        loss_m.update(loss.item())\n        acc_m.update(acc)\n\n        batch_bar.set_postfix(\n            acc         = \"{:.04f}% ({:.04f})\".format(acc, acc_m.avg),\n            loss        = \"{:.04f} ({:.04f})\".format(loss.item(), loss_m.avg))\n\n        batch_bar.update()\n\n    batch_bar.close()\n    return acc_m.avg, loss_m.avg","metadata":{"id":"5qkdH295wNUX","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"gc.collect() # These commands help you when you face CUDA OOM error\ntorch.cuda.empty_cache()","metadata":{"id":"-Yan5vLDyj-3","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Verification Task","metadata":{"id":"0q1gRMAsyknz"}},{"cell_type":"code","source":"def valid_epoch_ver(model, pair_data_loader, device, config):\n\n    model.eval()\n    scores = []\n    match_labels = []\n    batch_bar = tqdm(total=len(pair_data_loader), dynamic_ncols=True, position=0, leave=False, desc='Val Veri.')\n    for i, (images1, images2, labels) in enumerate(pair_data_loader):\n\n        # match_labels = match_labels.to(device)\n        images = torch.cat([images1, images2], dim=0).to(device)\n        # Get model outputs\n        with torch.inference_mode():\n            outputs = model(images)\n\n        feats = F.normalize(outputs['feats'], dim=1)\n        feats1, feats2 = feats.chunk(2)\n        similarity = F.cosine_similarity(feats1, feats2)\n        scores.append(similarity.cpu().numpy())\n        match_labels.append(labels.cpu().numpy())\n        batch_bar.update()\n\n    scores = np.concatenate(scores)\n    match_labels = np.concatenate(match_labels)\n\n    FPRs=['1e-4', '5e-4', '1e-3', '5e-3', '5e-2']\n    metric_dict = get_ver_metrics(match_labels.tolist(), scores.tolist(), FPRs)\n    print(metric_dict)\n\n    return metric_dict['ACC']","metadata":{"id":"SSGeDCi-wa1W","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# WandB","metadata":{"id":"piblCbe5yotj"}},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nwandb_api = user_secrets.get_secret(\"WANDB_API\")\n\nwandb.login(key=wandb_api) # API Key is in your wandb account, under settings (wandb.ai/settings)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Create your wandb run\nrun = wandb.init(\n    # name = \"early-submission\", ## Wandb creates random run names if you skip this field\n    # reinit = True, ### Allows reinitalizing runs when you re-run this cell\n    # run_id = ### Insert specific run id here if you want to resume a previous run\n    # resume = \"must\" ### You need this to resume previous runs, but comment out reinit = True when using this\n    project = \"hw2p2-ranais\", ### Project should be created in your wandb account\n    config = config ### Wandb Config for your run\n)","metadata":{"id":"GLNNqwV4ysNP","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"wandb.save(\"all\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Checkpointing and Loading Model","metadata":{"id":"t0RrtpFKzH3k"}},{"cell_type":"code","source":"import os\ncheckpoint_dir = config['checkpoint_dir']\n\n# Create the directory if it doesn't exist\nos.makedirs(checkpoint_dir, exist_ok=True)","metadata":{"id":"O1gbAkMtlWHk","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def save_model(model, optimizer, scheduler, metrics, epoch, path):\n    torch.save(\n        {'model_state_dict'         : model.state_dict(),\n         'optimizer_state_dict'     : optimizer.state_dict(),\n         'scheduler_state_dict'     : scheduler.state_dict(),\n         'metric'                   : metrics,\n         'epoch'                    : epoch},\n         path)\n\n\ndef load_model(model, optimizer=None, scheduler=None, path='./checkpoint.pth'):\n    checkpoint = torch.load(path)\n    model.load_state_dict(checkpoint['model_state_dict'])\n    if optimizer is not None:\n        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n    else:\n        optimizer = None\n    if scheduler is not None:\n        scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n    else:\n        scheduler = None\n    epoch = checkpoint['epoch']\n    metrics = checkpoint['metric']\n    return model, optimizer, scheduler, epoch, metrics","metadata":{"id":"dDFmC8hpzLOq","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Experiments","metadata":{"id":"wpFT7iriy5bi"}},{"cell_type":"code","source":"e = 0\nbest_valid_cls_acc = 0.0\neval_cls = True\nbest_valid_ret_acc = 0.0\nfor epoch in range(e, config['epochs']):\n        # epoch\n        print(\"\\nEpoch {}/{}\".format(epoch+1, config['epochs']))\n\n        # train\n        train_cls_acc, train_loss = train_epoch(model, cls_train_loader, optimizer, scheduler, scaler, DEVICE, config)\n        curr_lr = float(optimizer.param_groups[0]['lr'])\n        print(\"\\nEpoch {}/{}: \\nTrain Cls. Acc {:.04f}%\\t Train Cls. Loss {:.04f}\\t Learning Rate {:.04f}\".format(epoch + 1, config['epochs'], train_cls_acc, train_loss, curr_lr))\n        metrics = {\n            'train_cls_acc': train_cls_acc,\n            'train_loss': train_loss,\n        }\n        # classification validation\n        if eval_cls:\n            valid_cls_acc, valid_loss = valid_epoch_cls(model, cls_val_loader, DEVICE, config)\n            print(\"Val Cls. Acc {:.04f}%\\t Val Cls. Loss {:.04f}\".format(valid_cls_acc, valid_loss))\n            metrics.update({\n                'valid_cls_acc': valid_cls_acc,\n                'valid_loss': valid_loss,\n            })\n\n        # retrieval validation\n        valid_ret_acc = valid_epoch_ver(model, ver_val_loader, DEVICE, config)\n        print(\"Val Ret. Acc {:.04f}%\".format(valid_ret_acc))\n        metrics.update({\n            'valid_ret_acc': valid_ret_acc\n        })\n\n        # save model\n        save_model(model, optimizer, scheduler, metrics, epoch, os.path.join(config['checkpoint_dir'], 'last.pth'))\n        print(\"Saved epoch model\")\n\n        # save best model\n        if eval_cls:\n            if valid_cls_acc >= best_valid_cls_acc:\n                best_valid_cls_acc = valid_cls_acc\n                save_model(model, optimizer, scheduler, metrics, epoch, os.path.join(config['checkpoint_dir'], 'best_cls.pth'))\n                wandb.save(os.path.join(config['checkpoint_dir'], 'best_cls.pth'))\n                print(\"Saved best classification model\")\n\n        if valid_ret_acc >= best_valid_ret_acc:\n            best_valid_ret_acc = valid_ret_acc\n            save_model(model, optimizer, scheduler, metrics, epoch, os.path.join(config['checkpoint_dir'], 'best_ret.pth'))\n            wandb.save(os.path.join(config['checkpoint_dir'], 'best_ret.pth'))\n            print(\"Saved best retrieval model\")\n\n        # log to tracker\n        if run is not None:\n            run.log(metrics)","metadata":{"id":"59FcCeJfy3Zm","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Testing and Kaggle Submission","metadata":{"id":"VXLTfQjv0cCb"}},{"cell_type":"code","source":"def test_epoch_ver(model, pair_data_loader, config):\n\n    model.eval()\n    scores = []\n    batch_bar = tqdm(total=len(pair_data_loader), dynamic_ncols=True, position=0, leave=False, desc='Val Veri.')\n    for i, (images1, images2) in enumerate(pair_data_loader):\n\n        images = torch.cat([images1, images2], dim=0).to(DEVICE)\n        # Get model outputs\n        with torch.inference_mode():\n            outputs = model(images)\n\n        feats = F.normalize(outputs['feats'], dim=1)\n        feats1, feats2 = feats.chunk(2)\n        similarity = F.cosine_similarity(feats1, feats2)\n        scores.extend(similarity.cpu().numpy().tolist())\n        batch_bar.update()\n\n    return scores","metadata":{"id":"XUAa3m2h0eCD","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"scores = test_epoch_ver(model, ver_test_loader, config)","metadata":{"id":"sZnuyzfD5xdS","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"with open(\"verification_early_submission.csv\", \"w+\") as f:\n    f.write(\"ID,Label\\n\")\n    for i in range(len(scores)):\n        f.write(\"{},{}\\n\".format(i, scores[i]))","metadata":{"id":"fOLRdaKZ50RQ","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"### Finish your wandb run\nrun.finish()","metadata":{"id":"Gj8RkRXtAx3-","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**For Kaggle users:**\n\nYou'll find the \"**verification_submission.csv**\" under the **right sidebar → Output → kaggle/working/**.\nIf you don't see it immediately, click the refresh icon in the top-right corner.\nOnce located, **right-click → Download**, and then manually upload the file to the Kaggle competition submission page.\n\n**For Colab and PSC users**, run the following blocks:","metadata":{"id":"MMIoBN4vlRE6"}},{"cell_type":"code","source":"os.environ['KAGGLE_USERNAME'] = 'cmuranais'\nos.environ['KAGGLE_KEY'] = user_secrets.get_secret(\"KAGGLE_API\")\n\nimport kaggle","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"### Submit to kaggle competition using kaggle API (Uncomment below to use)\n!kaggle competitions submit -c 11785-hw-2-p-2-face-verification-fall-2025 -f verification_early_submission.csv -m \"Test Submission\"\n\n### However, its always safer to download the csv file and then upload to kaggle","metadata":{"id":"v3_8VUTQ52zT","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# If you need to download the CSV and submit manually, uncomment the appropriate section below.\n\n# #Colab users:\n# from google.colab import files\n# files.download(\"verification_submission.csv\")\n\n# #PSC users:\n# Download the file manually:\n# # Navigate to the left sidebar → locate the file in \"/jet/home/<your_username>\"\n# # Right-click the file and select \"Download\".\n# # If you don't see it immediately, try refreshing the file browser.","metadata":{"id":"9nEZR3WNU2am","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### TODO: Generate a model_metadata.json file to save your model's data (due 48 hours after Kaggle submission deadline OR the day of slack submission)","metadata":{"id":"8qQQ3MWmNIiz"}},{"cell_type":"code","source":"import json, os, sys, torch, datetime\n################################\n# TODO: Keep the model_metadata.json\n# file safe for submission ater.\n################################\ndef is_colab():\n    return \"google.colab\" in sys.modules and \"COLAB_GPU\" in os.environ\n\ndef is_kaggle():\n    return \"KAGGLE_KERNEL_RUN_TYPE\" in os.environ or \"KAGGLE_URL_BASE\" in os.environ\n\ndef generate_model_submission_file(model):\n    timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M\")\n    json_filename = f\"model_metadata_{timestamp}.json\"\n\n    # Create JSON with parameter count, model architecture, and predictions\n    output_json = {\n        \"parameter_count\": sum(p.numel() for p in model.parameters() if p.requires_grad),\n        \"model_architecture\": str(model),\n    }\n\n    # Save metadata JSON\n    with open(json_filename, \"w\") as f:\n        json.dump(output_json, f, indent=2)\n\n    # Download / display link depending on environment\n    if is_colab():\n        from google.colab import files\n        print(f\"OK: Saved as {json_filename}. Downloading in Colab...\")\n        files.download(json_filename)\n\n    elif is_kaggle():\n        from IPython.display import FileLink, display\n        print(\"#\" * 100)\n        print(f\"OK: Your submission file `{json_filename}` has been generated.\")\n        print(\"TODO: Click the link below.\")\n        print(\"1. The file will open in a new tab.\")\n        print(\"2. Right-click anywhere in the new tab and select 'Save As...'\")\n        print(\"3. Save the file to your computer with the `.json` extension.\")\n        print(\"You MUST submit this file to Autolab if this is your best submission.\")\n        print(\"#\" * 100 + \"\\n\")\n        display(FileLink(json_filename))\n\n    else:\n        print(f\"OK: saved model data saved to: '{json_filename}'\")\n        print(\"REQUIRED to submit to Autolab if these are the best model weights.\")\n\ngenerate_model_submission_file(model)\n#### IMPORTANT: Do NOT change the name of the model_metadata_....json file!!","metadata":{"id":"DDA0hFlcqhtA","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## TODO: fill in your submission requirements","metadata":{"id":"vxIHmPMj3vji"}},{"cell_type":"markdown","source":"### Notes:\n\n- You will need to set the root path to your submission files (eg. MODEL_METADATA_JSON, NOTEBOOK_PATH). This will depend on your setup. For eg. if you are following our setup instruction:\n  - `Colab:`: `\"/content/...\"`In the left file pane, right-click the desired file or folder and select “Copy path”.\n  - `Kaggle:`: `\"/kaggle/working/...\"` In the right sidebar, hover over the target file or folder and click the “copy directory path” icon.\n  - `PSC`: `\"/jet/home/<your_username>/...\"` You can check the files in this path by running: ```!ls /jet/home/<your_username>/```\n\n\nKindly modify your configurations to suit your ablations and be keen to include your name.","metadata":{"id":"4cYYFv9xx84b"}},{"cell_type":"code","source":"SUBMIT = False\nif SUBMIT:\n\n    ####################################\n    #             README\n    ####################################\n    \n    # TODO: Please complete all components of this README\n    README = \"\"\"\n    - **Model**: Model archtiecture description. Anything unique? Any specific architecture shapes or strategies?\n    - **Training Strategy**: optimizer + scheduler + loss function + any other unique ideas\n    - **Augmentations**: augmentations if used. If augmentations weren't used, then ignore\n    - **Notebook Execution**: Any instructions required to run your notebook.\n    \"\"\"\n    \n    ####################################\n    #       Credentials (Optional)\n    ####################################\n    \n    # These are not required **IF** you have run the cells to declare these variables above.\n    # If you would like to paste your credentials here again, feel free to:\n    # OPTIONAL: Fill these out if you do not want to re-run previous cells to re-initialize these credential variables\n    \n    KAGGLE_USERNAME = \"todo-kaggle username\" #TODO\n    KAGGLE_API_KEY = \"todo-kaggle key\" #TODO\n    WANDB_API_KEY = \"todo-wandb key\" #TODO\n    \n    \n    ####################################\n    #             Wandb Logs\n    ####################################\n    \n    # TODO: Your wandb project url should look like https://wandb.ai/username-or-team-name/project-name\n    #(Take these parameters and put them in the variables below)\n\n    WANDB_USERNAME_OR_TEAMNAME = \"todo-wandb username/teamname\" # TODO: Put your username-or-team-name here\n    WANDB_PROJECT = \"todo-wandb project name\" # TODO: Put your project-name\n    \n    ####################################\n    #         Notebook & Files\n    ####################################\n    \n    # TODO: Download HW2P2 Notebook (if on colab or kaggle) and upload both your HW2P2 notebook + model_metadata_*.json to your file system.\n    # TODO: For each file, obtain the file paths and put them below.\n    \n    # TODO: COLAB INSTRUCTIONS:\n    # * With Colab, upload your desired file (notebook or model_metadata.json) to \"Files\"\n    # * Right-click the file, click \"Copy Path,\"\n    # * Paste the path below.\n    \n    # TODO: KAGGLE INSTRUCTIONS:\n    # * First download a copy of your notebook with \"File > Download Notebook\"\n    # Then...\n    # * Click \"File\" in the top left of the screen\n    # * Go to \"Upload Input > Upload Model\"\n    # * Upload your notebook file.\n    # * For \"Model Name\" put HW2P2_Final_Submission\n    # * For \"Framework\" put \"Other\"\n    # * For \"License\" put \"Other\"\n    # * Click \"Upload another file\" and upload your model_metadata####.json file as well.\n    # * Now, on your right in your \"Models\" section, you should see a new folder with your submission files.\n    # * Click on the \"Copy File Path\" buttons for the notebook and json file and paste them below.\n    \n    # TODO: Linux system:\n    # * Simply upload or find the path of your notebook file and model_metadata###.json file, and paste them here.\n\n    NOTEBOOK_PATH = \"/content/drive/MyDrive/TA/hw2p2/HW2P2_TA_Starter_Notebook_final.ipynb\" # TODO: Put your HW2P2 notebook path here\n    MODEL_METADATA_JSON = \"/content/model_metadata_2025-07-14_21-42.json\" # TODO: Put your Model Metadata path json file here (see end of HW2P2 Code Notebook to get this file)\n    \n    \n    ####################################\n    #         Additional Files\n    ####################################\n    \n    ADDITIONAL_FILES = [ # TODO: Upload any files and add any paths to any additional files you would like to include in your submission, otherwise, leave this empty\n    ]\n    \n    ####################################\n    #         SLACK SUBMISSION\n    ####################################\n    \n    ENABLE_SLACK_SUBMISSION = False # TODO: Set this to true if you are submitting to the Slack competition\n    \n    ####################################\n    #     Creating the Submission\n    ####################################\n    \n    # TODO: Once the README, wandb information, and file paths are filled in, run this cell,\n    # run the \"Assignment Backend Functions\" in the next cells, and generate the final zip file at the end.\n    \n    SAFE_SUBMISSION = True # TODO: Set this to False if you want to generate a submission.zip even if you are missing files, otherwise it's recommended to keep this as True\n","metadata":{"id":"tqg6zK783t-N","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Assignment Backend Submission Functions (DO NOT MODIFY, just run these cells)","metadata":{"id":"zmsS-WutY2n_"}},{"cell_type":"code","source":"if SUBMIT:\n    from datetime import datetime\n    \n    ######################################\n    #       Assignment Configs\n    ######################################\n    \n    WANDB_METRIC = \"EER\"\n    WANDB_DIRECTION = \"descending\"\n    WANDB_TOP_N = 10\n    WANDB_OUTPUT_PKL = \"wandb_top_runs.pkl\"\n    \n    # Kaggle configuration\n    COMPETITION_NAME = \"11785-hw-2-p-2-face-verification-fall-2025\"\n    SLACK_COMPETITION_NAME = \"11785-hw-2-p-2-face-verification-fall-2025-slack\"\n    INAL_SUBMISSION_DATETIME = datetime.strptime(\"2025-10-12 23:55:00\", \"%Y-%m-%d %H:%M:%S\")\n    SLACK_SUBMISSION_DATETIME = datetime.strptime(\"2025-10-17 23:55:00\", \"%Y-%m-%d %H:%M:%S\")\n    GRADING_DIRECTION = \"descending\"\n    KAGGLE_OUTPUT_JSON = \"kaggle_data.json\"\n    \n    SUBMISSION_OUTPUT = \"HW2P2_final_submission.zip\"","metadata":{"id":"a1MFM7JBcDba","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"if SUBMIT:\n    from datetime import datetime, timezone\n    import zoneinfo\n    \n    eastern = zoneinfo.ZoneInfo(\"America/New_York\")\n    FINAL_DEADLINE_UTC = (\n        datetime.strptime(FINAL_SUBMISSION_DATETIME, \"%Y-%m-%d %H:%M:%S\")\n        .replace(tzinfo=eastern)\n        .astimezone(timezone.utc)\n    )\n    \n    SLACK_DEADLINE_UTC = (\n        datetime.strptime(SLACK_SUBMISSION_DATETIME, \"%Y-%m-%d %H:%M:%S\")\n        .replace(tzinfo=eastern)\n        .astimezone(timezone.utc)\n    )\n\n    ACKNOWLEDGEMENT_MESSAGE = \"\"\"\n    Submission of this file and assignment indicate the student's agreement to the following Aknowledgement requirements:\n    Setting the ACNKOWLEDGED flag to True indicates full understanding and acceptance of the following:\n    1. Slack days may ONLY be used on P2 FINAL (not checkpoint) submission. I.e. you may use slack days to submit final P2 kaggle scores (such as this one) later on the **SLACK KAGGLE COMPETITION** at the expense of your Slack days.\n    2. The final autolab **code submission is due 48 hours after** the conclusion of the Kaggle Deadline (or, the same day as your final kaggle submission).\n    3. Course staff will require your kaggle username here, and then will pull your official PRIVATE kaggle leaderboard score. This submission may result in slight variance in scores/code, but we will check for acceptable discrepancies. Any discrepancies related to modifying the submission code (at the bottom of the notebook) will result in an AIV.\n    4. You are NOT allowed to use any code that will pre-load models (such as those from Hugging Face, etc.).\n       You MAY use models described by papers or articles, but you MUST implement them yourself through fundamental PyTorch operations (i.e. Linear, Conv2d, etc.).\n    5. You are NOT allowed to use any external data/datasets at ANY point of this assignment.\n    6. You may work with teammates to run ablations/experiments, BUT you must submit your OWN code and your OWN results.\n    7. Failure to comply with the prior rules will be considered an Academic Integrity Violation (AIV).\n    8. Late submissions MUST be submitted through the Slack Kaggle (see writeup for details). Any submissions made to the regular Kaggle after the original deadline will NOT be considered, no matter how many slack days remain for the student.\n    \"\"\"\n    def save_acknowledgment_file():\n        if ACKNOWLEDGED:\n            with open(\"acknowledgement.txt\", \"w\") as f:\n                f.write(ACKNOWLEDGEMENT_MESSAGE.strip())\n            print(\"Saved acknowledgement.txt\")\n            return True\n        else:\n            print(\"ERROR: Must set ACKNOWLEDGED = True.\")\n            return False\n    # Saves README\n    def save_readme(readme):\n        try:\n            with open(\"README.txt\", \"w\") as f:\n                f.write(readme.strip())\n    \n            print(\"Saved README.txt\")\n        except Exception as e:\n            print(f\"ERROR: Error occured while saving README.txt: {e}\")\n            return False\n    \n        return True\n    \n    # Saves wandb logs\n    import wandb, json, pickle\n    \n    def save_top_wandb_runs():\n        wandb.login(key=WANDB_API_KEY)\n        if not ACKNOWLEDGED:\n            print(\"ERROR: Must set ACKNOWLEDGED = True.\")\n            return False\n    \n        api = wandb.Api()\n        runs = api.runs(\n            f\"{WANDB_USERNAME_OR_TEAMNAME}/{WANDB_PROJECT}\",\n            order=f\"{'-' if WANDB_DIRECTION == 'descending' else ''}summary_metrics.{WANDB_METRIC}\"\n        )\n        selected_runs = runs[:min(WANDB_TOP_N, len(runs))]\n    \n        if not selected_runs:\n            print(f\"ERROR: No runs found for {WANDB_USERNAME_OR_TEAMNAME}/{WANDB_PROJECT}. Please check that your wandb credentials (Wandb Username/Team Name, API Key, and Project Name) are correct.\")\n            return False\n    \n        all_data = []\n        for run in selected_runs:\n            run_data = {\n                \"id\": run.id,\n                \"name\": run.name,\n                \"tags\": run.tags,\n                \"state\": run.state,\n                \"created_at\": str(run.created_at),\n                \"config\": run.config,\n                \"summary\": dict(run.summary),\n            }\n            try:\n                run_data[\"history\"] = run.history(samples=1000)\n            except Exception as e:\n                run_data[\"history\"] = f\"Failed to fetch history: {str(e)}\"\n            all_data.append(run_data)\n        with open(WANDB_OUTPUT_PKL, \"wb\") as f:\n            pickle.dump(all_data, f)\n    \n        print(f\"OK: Exported {len(all_data)} WandB runs to {WANDB_OUTPUT_PKL}\")\n    \n        return True\n    # Saves kaggle information\n    \n    # Install dependencies silently (only if running on Colab)\n    import sys\n    \n    from datetime import datetime\n    import os, json, requests\n    def kaggle_login(username, key):\n        os.makedirs(os.path.expanduser(\"~/.kaggle\"), exist_ok=True)\n        with open(os.path.expanduser(\"~/.kaggle/kaggle.json\"), \"w\") as f:\n            json.dump({\"username\": username, \"key\": key}, f)\n        os.chmod(os.path.expanduser(\"~/.kaggle/kaggle.json\"), 0o600)\n    \n    \n    def get_active_submission_config():\n        if ENABLE_SLACK_SUBMISSION:\n            return SLACK_COMPETITION_NAME, SLACK_DEADLINE_UTC\n        return COMPETITION_NAME, FINAL_DEADLINE_UTC\n    \n    def kaggle_user_exists(usernagbme):\n        try:\n            return requests.get(f\"https://www.kaggle.com/{KAGGLE_USERNAME}\").status_code == 200\n        except Exception as e:\n            print(f\"ERROR: Error occured while checking Kaggle user: {e}\")\n            return False\n    \n        DEFAULT_SCORE=0\n    if GRADING_DIRECTION == \"ascending\":\n        DEFAULT_SCORE=0\n    else:\n        DEFAULT_SCORE=1.0\n    \n    def get_best_kaggle_score(subs):\n        def extract_score(s): return float(s.private_score or s.public_score or DEFAULT_SCORE)\n        if not subs:\n            return None, None\n        best = max(subs, key=lambda s: extract_score(s) if GRADING_DIRECTION == \"ascending\" else -extract_score(s))\n    \n        score_type = \"private\" if best.private_score not in [None, \"\"] else \"public\"\n        return extract_score(best), score_type\n    \n    def save_kaggle_json(kaggle_username, kaggle_key):\n    \n        kaggle_login(kaggle_username, kaggle_key)\n    \n        from kaggle.api.kaggle_api_extended import KaggleApi\n    \n        if not ACKNOWLEDGED:\n            print(\"ERROR: Must set ACKNOWLEDGED = True.\")\n            return False\n    \n        if not kaggle_user_exists(KAGGLE_USERNAME):\n            print(f\"ERROR: User '{KAGGLE_USERNAME}' not found.\")\n            return False\n    \n        comp_name, deadline = get_active_submission_config()\n    \n        api = KaggleApi()\n        api.authenticate()\n    \n        # Get competition submissions\n        submissions = [s for s in api.competition_submissions(comp_name) if getattr(s, \"_submitted_by\", None) == KAGGLE_USERNAME]\n        if not submissions:\n            print(f\"ERROR: No valid submissions found for user [{KAGGLE_USERNAME}] for this competition [{comp_name}]. Slack flag set to [{ENABLE_SLACK_SUBMISSION}]\")\n            print(\"Please double check your Kaggle username and ensure you've submitted at least once.\")\n            return False\n    \n        score, score_type = get_best_kaggle_score(submissions)\n        result = {\n            \"kaggle_username\": KAGGLE_USERNAME,\n            \"acknowledgement\": ACKNOWLEDGED,\n            \"submitted_slack\": ENABLE_SLACK_SUBMISSION,\n            \"competition_name\": comp_name,\n            \"deadline\": deadline.strftime(\"%Y-%m-%d %H:%M:%S\"),\n            \"raw_score\": score * 100.0,\n            \"score_type\": score_type,\n        }\n    \n        print(f\"OK: Projected score (excluding bonuses) saved as {KAGGLE_OUTPUT_JSON}\")\n        if score:\n            print(f\"Best score {score}.\")\n            with open(KAGGLE_OUTPUT_JSON, \"w\") as f:\n                json.dump(result, f, indent=2)\n            return True\n        return False\n    \n    import os\n    import sys\n    import zipfile\n\n    \n    def create_submission_zip(additional_files, safe_flag):\n        if not \"ACKNOWLEDGED\" in globals() or not ACKNOWLEDGED:\n            print(\"ERROR: Make sure to RUN the Acknowledgement cell (at the top of the notebook). Also, must set ACKNOWLEDGED = True.\")\n            return\n    \n        if (not save_acknowledgment_file()):\n            print(\"ERROR: Make sure to RUN the Acknowledgement cell (at the top of the notebook). Also, must set ACKNOWLEDGED = True.\")\n            return\n    \n    \n        if not \"ENABLE_SLACK_SUBMISSION\" in globals() or ENABLE_SLACK_SUBMISSION is None:\n            print(\"ERROR: \\\"ENABLE_SLACK_SUBMISSION\\\" variable is not defined. \\nTODO: Make sure to RUN the cell (A few cells up at the beginning of the submission section). \\nMake sure to set the ENABLE_SLACK_SUBMISSION checkbox if you're on colab, or set the parameter correctly set on other platforms \\n(if you are submitting through the SLACK submission).\")\n            return\n    \n        if not \"README\" in globals() or not README:\n            print(\"ERROR: Make sure to RUN the README cell(above your credentials cell).\")\n            return\n    \n        if (not save_readme(README)):\n            print(\"ERROR: Error while saving the README file. Make sure to complete and RUN the README cell(above your credentials cell).\")\n            return\n    \n        if (not save_top_wandb_runs()):\n            return\n    \n        if not \"KAGGLE_USERNAME\" in globals() or not \"KAGGLE_API_KEY\" in globals() or not KAGGLE_USERNAME or not KAGGLE_API_KEY:\n            print(\"ERROR: Make sure to set KAGGLE_USERNAME and KAGGLE_API_KEY for this code submission.\")\n            return\n    \n        if (not save_kaggle_json(KAGGLE_USERNAME, KAGGLE_API_KEY)):\n            print(f\"ERROR: An error occured while retrieve kaggle information from username [{KAGGLE_USERNAME}] from competition [{get_active_submission_config()[0]}] with slack flag set to [{ENABLE_SLACK_SUBMISSION}]. Please check your kaggle username, key, and submission.\")\n            return\n    \n        files_to_zip = [\n            \"acknowledgement.txt\",\n            \"README.txt\",\n            KAGGLE_OUTPUT_JSON,\n            WANDB_OUTPUT_PKL,\n            MODEL_METADATA_JSON,\n            NOTEBOOK_PATH,\n        ] + additional_files\n    \n        missing_files = False\n    \n        with zipfile.ZipFile(SUBMISSION_OUTPUT, \"w\") as zipf:\n            for file_path in files_to_zip:\n                if os.path.exists(file_path):\n                    arcname = os.path.basename(file_path)  # flatten path\n                    zipf.write(file_path, arcname=arcname)\n                    print(f\"OK: Added {arcname}\")\n                else:\n                    missing_files = True\n                    print(f\"ERROR: Missing file: {file_path}\")\n    \n        if missing_files:\n            if safe_flag:\n                raise \"ERROR: Missing files with safety flag set to True. Please upload any necessary files, ensure you have the correct paths and rerun all cells.\"\n            else:\n                print(\"WARNING: Missing files with safety flag set to False. Submission may be incomplete.\")\n    \n        if \"google.colab\" in sys.modules:\n            from google.colab import files\n            files.download(SUBMISSION_OUTPUT)\n    \n        print(\"Final submission saved as:\", SUBMISSION_OUTPUT)","metadata":{"id":"zb-OGuTN5MI5","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# File Generation (TODO: Check file generation outputs for any errors)","metadata":{"id":"0m2dFq8pc21u"}},{"cell_type":"markdown","source":"### For Colab and PSC users:","metadata":{"id":"O7kUDNxw0Iw7"}},{"cell_type":"code","source":"if SUBMIT: \n    create_submission_zip(ADDITIONAL_FILES, SAFE_SUBMISSION)\n\n#TODO: If the HW2P2_final_submission.zip file does not\n# automatically bring up a donwload pop-up\n# Then make sure to entire the files and\n#manually download the checkpoint_submission.json file.","metadata":{"id":"Ii_kBJ7KZAYm","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### For Kaggle users:\nManually download the \"**HW2P2_final_submission.zip**\" file from the right sidebar under the **kaggle/working/.. **directory.\n\n","metadata":{"id":"b9HAE4-r0oZn"}}]}